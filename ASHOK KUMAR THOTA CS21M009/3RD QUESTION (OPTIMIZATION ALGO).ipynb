{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcLcjwAwvKsE"
      },
      "source": [
        "## Used Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id7DewAiu4b8"
      },
      "outputs": [],
      "source": [
        "#Real Code\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from tqdm import tqdm_notebook \n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuleYS2EgLov"
      },
      "source": [
        "## Setting random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l0WUpNwgKs8"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaPcBEBvDTV"
      },
      "source": [
        "## Splitting and Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_41Yo1TUvAmT",
        "outputId": "c87dea10-40b0-4c88-d502-f7f13815a5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,) <class 'numpy.ndarray'> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "(60000, 784) (10000, 784) (60000,) (10000,) <class 'numpy.float64'> 0.0\n"
          ]
        }
      ],
      "source": [
        "(x_train,y_train),(x_val,y_val)=fashion_mnist.load_data()\n",
        "x_train = x_train.astype('float')/ 255.0\n",
        "x_val = x_val.astype('float')/ 255.0\n",
        "print(x_train.shape, x_val.shape,y_train.shape,y_val.shape,type(x_train[0][0]),x_train[0][0])\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_val = x_val.reshape(10000,784)\n",
        "print(x_train.shape, x_val.shape,y_train.shape,y_val.shape,type(x_train[0][0]),x_train[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAIK_sFreaCs"
      },
      "source": [
        "## 10% for testing from training data (Terminology: I have kept aside validation data as hidden data and considered test data for testing accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGMc3N5veXab",
        "outputId": "ad0f24ec-861e-4259-eb90-bb387c79737b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(54000, 784) (6000, 784) (54000,) (6000,) <class 'numpy.float64'> 0.0\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1,stratify=y_train, random_state=0)\n",
        "print(x_train.shape, x_test.shape,y_train.shape,y_test.shape,type(x_train[0][0]),x_train[0][0])\n",
        "print(type(x_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm9JTddcvfv7"
      },
      "source": [
        "## One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPGyZwbQvfC2",
        "outputId": "e81833de-5613-4ab6-f745-16f03e0a2b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(54000, 10) (6000, 10)\n"
          ]
        }
      ],
      "source": [
        "enc = OneHotEncoder()\n",
        "# 0 -> (1, 0, 0, 0), 1 -> (0, 1, 0, 0), 2 -> (0, 0, 1, 0), 3 -> (0, 0, 0, 1)\n",
        "y_OH_train = enc.fit_transform(np.expand_dims(y_train,1)).toarray()\n",
        "y_OH_val = enc.fit_transform(np.expand_dims(y_test,1)).toarray()\n",
        "print(y_OH_train.shape, y_OH_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rs2wI3svlsz"
      },
      "source": [
        "## Main class (Fully Vectorized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlaETVm_votL"
      },
      "outputs": [],
      "source": [
        "class FFSN_MultiClass:\n",
        "  \n",
        "  def __init__(self, n_inputs, n_outputs, hidden_sizes=[3]):\n",
        "    self.nx = n_inputs\n",
        "    self.ny = n_outputs\n",
        "    self.nh = len(hidden_sizes)\n",
        "    self.sizes = [self.nx] + hidden_sizes + [self.ny] \n",
        "\n",
        "    self.W = {}\n",
        "    self.B = {}\n",
        "    for i in range(self.nh+1):\n",
        "      self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n",
        "      self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n",
        "      \n",
        "  def sigmoid(self, x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "  \n",
        "  def softmax(self, x):\n",
        "    exps = np.exp(x)\n",
        "    return exps / np.sum(exps, axis=1).reshape(-1,1) \n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    self.A = {}\n",
        "    self.H = {}\n",
        "    self.H[0] = x\n",
        "    for i in range(self.nh):\n",
        "      self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1]\n",
        "      self.H[i+1] = self.sigmoid(self.A[i+1])\n",
        "    self.A[self.nh+1] = np.matmul(self.H[self.nh], self.W[self.nh+1]) + self.B[self.nh+1]\n",
        "    self.H[self.nh+1] = self.softmax(self.A[self.nh+1])\n",
        "    return self.H[self.nh+1]\n",
        "  \n",
        "  def predict(self, X):\n",
        "    Y_pred = []\n",
        "    for x in X:\n",
        "      y_pred = self.forward_pass(x)\n",
        "      Y_pred.append(y_pred)\n",
        "    return np.array(Y_pred).squeeze()\n",
        " \n",
        "  def grad_sigmoid(self, x):\n",
        "    return x*(1-x) \n",
        "  \n",
        "  def cross_entropy(self,label,pred):\n",
        "    yl=np.multiply(pred,label)\n",
        "    yl=yl[yl!=0]\n",
        "    yl=-np.log(yl)\n",
        "    yl=np.mean(yl)\n",
        "    return yl\n",
        " \n",
        "  def grad(self, x, y):\n",
        "    self.forward_pass(x)\n",
        "    self.dW = {}\n",
        "    self.dB = {}\n",
        "    self.dH = {}\n",
        "    self.dA = {}\n",
        "    L = self.nh + 1\n",
        "    self.dA[L] = (self.H[L] - y)\n",
        "    for k in range(L, 0, -1):\n",
        "      self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])\n",
        "      self.dB[k] = np.sum(self.dA[k],axis=0).reshape(1,-1) #self.dA[k]\n",
        "      self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)\n",
        "      self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1]))  \n",
        "    \n",
        "  def fit(self, X, Y, algo= \"GD\", epochs=100, initialize='True', learning_rate=0.01, display_loss=False,\n",
        "          mini_batch_size=60000, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9):\n",
        "      \n",
        "    if display_loss:\n",
        "      loss = {}\n",
        "    V_W = {}\n",
        "    V_B = {}\n",
        "    M_W = {}\n",
        "    M_B = {}\n",
        "    num_updates = 0\n",
        "    if initialize:\n",
        "      for i in range(self.nh+1):\n",
        "        V_W[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n",
        "        V_B[i+1] = np.zeros((1, self.sizes[i+1]))\n",
        "        M_W[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n",
        "        M_B[i+1] = np.zeros((1, self.sizes[i+1]))\n",
        "    if algo == 'SGD':\n",
        "        for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "          points_seen = 0\n",
        "          m = 1\n",
        "          mini_batch_size = 1\n",
        "          while(points_seen<X.shape[0]):\n",
        "            self.grad(X[points_seen:points_seen+mini_batch_size,:], Y[points_seen:points_seen+mini_batch_size,:])\n",
        "            m = mini_batch_size\n",
        "            for i in range(self.nh+1):\n",
        "              self.W[i+1] -= learning_rate * (self.dW[i+1]/m)\n",
        "              self.B[i+1] -= learning_rate * (self.dB[i+1]/m)\n",
        "            points_seen=points_seen+mini_batch_size           \n",
        "          if display_loss:\n",
        "            Y_pred = self.predict(X) \n",
        "            loss[epoch] = self.cross_entropy(Y, Y_pred)   \n",
        "    \n",
        "    if algo == 'Momentum':\n",
        "      for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "        points_seen = 0\n",
        "        m = mini_batch_size\n",
        "        while(points_seen<X.shape[0]):\n",
        "          self.grad(X[points_seen:points_seen+mini_batch_size,:], Y[points_seen:points_seen+mini_batch_size,:])\n",
        "          for i in range(self.nh+1):\n",
        "            V_W[i+1] = gamma * V_W[i+1] + learning_rate * (self.dW[i+1]/m)\n",
        "            V_B[i+1] = gamma * V_B[i+1] + learning_rate * (self.dB[i+1]/m)\n",
        "                    \n",
        "          for i in range(self.nh+1):\n",
        "            self.W[i+1] -= V_W[i+1]\n",
        "            self.B[i+1] -= V_B[i+1]\n",
        "          points_seen=points_seen+mini_batch_size\n",
        "            \n",
        "        if display_loss:\n",
        "          Y_pred = self.predict(X) \n",
        "          loss[epoch] = self.cross_entropy(Y, Y_pred)\n",
        "\n",
        "    if algo == 'NAG':\n",
        "      for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "        points_seen = 0\n",
        "        m = mini_batch_size\n",
        "        for i in range(self.nh+1):\n",
        "          self.W[i+1] -= gamma*V_W[i+1]\n",
        "          self.B[i+1] -= gamma*V_B[i+1]\n",
        "        while(points_seen<X.shape[0]):\n",
        "          self.grad(X[points_seen:points_seen+mini_batch_size,:], Y[points_seen:points_seen+mini_batch_size,:])\n",
        "          for i in range(self.nh+1):\n",
        "            self.W[i+1] -= learning_rate * (self.dW[i+1]/m)\n",
        "            self.B[i+1] -= learning_rate * (self.dB[i+1]/m)\n",
        "          for i in range(self.nh+1):\n",
        "            V_W[i+1] = gamma*V_W[i+1] + learning_rate * (self.dW[i+1]/m)\n",
        "            V_B[i+1] = gamma*V_B[i+1] + learning_rate * (self.dB[i+1]/m)\n",
        "            self.W[i+1] -= gamma*V_W[i+1]\n",
        "            self.B[i+1] -= gamma*V_B[i+1]\n",
        "          points_seen=points_seen+mini_batch_size \n",
        "        if display_loss:\n",
        "          Y_pred = self.predict(X) \n",
        "          loss[epoch] = self.cross_entropy(Y, Y_pred)  \n",
        "\n",
        "    if algo == 'RMSProp':\n",
        "      for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "        points_seen = 0\n",
        "        m = mini_batch_size\n",
        "        while(points_seen<X.shape[0]):\n",
        "          self.grad(X[points_seen:points_seen+mini_batch_size,:], Y[points_seen:points_seen+mini_batch_size,:])\n",
        "          for i in range(self.nh+1):\n",
        "            V_W[i+1] = beta*V_W[i+1] + (1-beta) * ((self.dW[i+1]/m)**2)\n",
        "            V_B[i+1] = beta*V_B[i+1] + (1-beta) * ((self.dB[i+1]/m)**2)\n",
        "            self.W[i+1] -= (learning_rate / np.sqrt(V_W[i+1]+ eps))* (self.dW[i+1]/m)\n",
        "            self.B[i+1] -= (learning_rate / np.sqrt(V_B[i+1]+ eps))* (self.dB[i+1]/m)\n",
        "          points_seen=points_seen+mini_batch_size\n",
        "        if display_loss:\n",
        "          Y_pred = self.predict(X) \n",
        "          loss[epoch] = self.cross_entropy(Y, Y_pred)\n",
        "    if algo == 'Adam':\n",
        "      for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "        points_seen = 0\n",
        "        m = mini_batch_size\n",
        "        while(points_seen<X.shape[0]):\n",
        "          self.grad(X[points_seen:points_seen+mini_batch_size,:], Y[points_seen:points_seen+mini_batch_size,:])\n",
        "          for i in range(self.nh+1):\n",
        "            num_updates += 1\n",
        "            M_W[i+1] = beta1*M_W[i+1] + (1-beta1) * (self.dW[i+1]/m)\n",
        "            M_B[i+1] = beta1*M_B[i+1] + (1-beta1) * (self.dB[i+1]/m)\n",
        "            V_W[i+1] = beta2*V_W[i+1] + (1-beta2) * ((self.dW[i+1]/m)**2)\n",
        "            V_B[i+1] = beta2*V_B[i+1] + (1-beta2) * ((self.dB[i+1]/m)**2)\n",
        "            M_W_C = M_W[i+1] / (1 - np.power(beta1, num_updates))\n",
        "            M_B_C = M_B[i+1] / (1 - np.power(beta1, num_updates))\n",
        "            V_W_C = V_W[i+1] / (1 - np.power(beta2, num_updates))\n",
        "            V_B_C = V_B[i+1] / (1 - np.power(beta2, num_updates))\n",
        "            self.W[i+1] -= (learning_rate / np.sqrt(V_W_C + eps))* M_W_C\n",
        "            self.B[i+1] -= (learning_rate / np.sqrt(V_B_C + eps)) * M_B_C\n",
        "          points_seen=points_seen+mini_batch_size\n",
        "        \n",
        "        if display_loss:\n",
        "          Y_pred = self.predict(X)\n",
        "          loss[epoch] = self.cross_entropy(Y, Y_pred)\n",
        "\n",
        "    if algo == 'Nadam':\n",
        "      for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
        "        points_seen = 0\n",
        "        m = mini_batch_size\n",
        "        while(points_seen<X.shape[0]):\n",
        "          num_updates += 1\n",
        "          self.grad(X[points_seen:points_seen+mini_batch_size,:], Y[points_seen:points_seen+mini_batch_size,:])\n",
        "          for i in range(self.nh+1):\n",
        "            M_W[i+1] = beta1*M_W[i+1] + (1-beta1) * (self.dW[i+1]/m)\n",
        "            M_B[i+1] = beta1*M_B[i+1] + (1-beta1) * (self.dB[i+1]/m)\n",
        "            V_W[i+1] = beta2*V_W[i+1] + (1-beta2) * ((self.dW[i+1]/m)**2)\n",
        "            V_B[i+1] = beta2*V_B[i+1] + (1-beta2) * ((self.dB[i+1]/m)**2)\n",
        "            M_W_C = M_W[i+1] / (1 - np.power(beta1, num_updates))\n",
        "            M_B_C = M_B[i+1] / (1 - np.power(beta1, num_updates))\n",
        "            V_W_C = V_W[i+1] / (1 - np.power(beta2, num_updates))\n",
        "            V_B_C = V_B[i+1] / (1 - np.power(beta2, num_updates))\n",
        "            self.W[i+1] -= (learning_rate / np.sqrt(V_W_C + eps))* (beta1*M_W_C + (1-beta1)/(1-np.power(beta1,num_updates))* (self.dW[i+1]/m))\n",
        "            self.B[i+1] -= (learning_rate / np.sqrt(V_B_C + eps)) * (beta1*M_B_C + (1-beta1)/(1-np.power(beta1,num_updates))* (self.dB[i+1]/m))\n",
        "          points_seen=points_seen+mini_batch_size\n",
        "        \n",
        "        if display_loss:\n",
        "          Y_pred = self.predict(X) \n",
        "          loss[epoch] = self.cross_entropy(Y, Y_pred)\n",
        "    \n",
        "    if display_loss:\n",
        "        plt.plot(np.array(list(loss.values())).astype(float))\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('CE')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpp0YLPqvyY8"
      },
      "source": [
        "## Creating Instance of the class and calling fit function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZmgl7-1vx2V"
      },
      "outputs": [],
      "source": [
        "ffsn_multi = FFSN_MultiClass(784,10,[128,128,128])\n",
        "ffsn_multi.fit(x_train,y_OH_train,epochs=10,learning_rate=.0005,display_loss=True,algo= \"Nadam\",mini_batch_size=128)\n",
        "#we can add new optimizer easily and it works with different batch sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYpMaAvs17QS"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOEPFb52v-5X",
        "outputId": "332ba717-fcf5-43f2-83c2-d8d5b077e4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training accuracy 0.86\n",
            "Validation accuracy 0.84\n"
          ]
        }
      ],
      "source": [
        "Y_pred_train = ffsn_multi.predict(x_train)\n",
        "Y_pred_train = np.argmax(Y_pred_train,1)\n",
        "\n",
        "Y_pred_val = ffsn_multi.predict(x_test)\n",
        "Y_pred_val = np.argmax(Y_pred_val,1)\n",
        "\n",
        "\n",
        "accuracy_train = accuracy_score(Y_pred_train, y_train)\n",
        "accuracy_val = accuracy_score(Y_pred_val, y_test)\n",
        "\n",
        "print(\"Training accuracy\", round(accuracy_train, 2))\n",
        "print(\"Validation accuracy\", round(accuracy_val, 2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL 1.3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}